{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Title:  Convolutional Neural Networks with Backpropagation model for Computer Vision (Python)\n",
    "Key Notes: \n",
    "1.\tImplementing Backpropagation algorithm.\n",
    "2.\tModel works as image classifier.\n",
    "3.\tconvolution, max pooling and average pooling using NumPy array operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the function of the form:\n",
    "\n",
    "$f(x,y)=\\frac{x+\\sigma(y)}{\\sigma(x)+(x+y)^2 }$\n",
    "\n",
    "The sigmoid function $\\sigma(x)$ is defined as:\n",
    "\n",
    "$\\sigma(x)=\\frac{1}{1+e^{-x}}=\\frac{e^x}{e^x+1}$\n",
    "\n",
    "Its gradient is:\n",
    "\n",
    "$\\frac{\\partial\\sigma(x)}{\\partial x}=(1-\\sigma(x))\\sigma(x)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the computational graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint1: We will define the computational graph for each node. The CG for the given function is then a combination of all nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hint2: Each node should be defined as a class. Then we can have multiple instances of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a class for addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first one is an example for you\n",
    "# No need to change it\n",
    "class Addition():\n",
    "    '''A simple addition node'''\n",
    "    \n",
    "    # The __init__ method is alwayed required for a class definition.\n",
    "    # It defines how to create an instance of this class\n",
    "    # It will be called each time an instance of this class is created\n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        self.gradient = 0\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        # the keyword self refers to the address of the instance\n",
    "        # a variable self.xx belongs to the instance\n",
    "        # it can be called be all methods of the class once initialized\n",
    "        # therefore you can use them for the backward operation\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.result = self.x + self.y\n",
    "        return self.result\n",
    "    \n",
    "    def backward(self, upstream_gradient):\n",
    "        return upstream_gradient # because local gradient = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a class for reciprocal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finish the class Reciprocal\n",
    "class Reciprocal():\n",
    "    '''A simple reciprocal node'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.result = 1 # not possible to have reciprocal=0\n",
    "        self.gradient = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.result = 1 / x\n",
    "        return self.result\n",
    "    \n",
    "    def backward(self, upstream_gradient):\n",
    "        local_gradient = -1 / (self.x * self.x)\n",
    "        self.gradient = upstream_gradient * local_gradient\n",
    "        return self.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a class for square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Square():\n",
    "    '''A simple square gate'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        self.gradient = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.result = self.x * self.x\n",
    "        return self.result\n",
    "    \n",
    "    def backward(self, upstream_gradient):\n",
    "        local_gradient = 2 * self.x\n",
    "        self.gradient = upstream_gradient * local_gradient\n",
    "        return self.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a class for sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # we need to use math.exp()\n",
    "\n",
    "class Sigmoid():\n",
    "    '''A simple sigmoid gate'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.result = 0.5\n",
    "        self.gradient = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        self.x = x\n",
    "        self.result = 1 / (1 + math.exp(-x))\n",
    "        return self.result\n",
    "    \n",
    "    def backward(self, upstream_gradient):\n",
    "        local_gradient = (1 - self.result) * self.result\n",
    "        self.gradient = upstream_gradient * local_gradient\n",
    "        return self.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define a class for multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiplication():\n",
    "    '''A simple mul gate'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.result = 0\n",
    "        self.gradient_x = 0\n",
    "        self.gradient_y = 0\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.result = self.x * self.y\n",
    "        return self.result\n",
    "    \n",
    "    def backward(self, upstream_gradient):\n",
    "        local_gradient_x = self.y\n",
    "        local_gradient_y = self.x\n",
    "        # gradients are different for the inputs\n",
    "        self.gradient = [upstream_gradient * local_gradient_x, upstream_gradient * local_gradient_y]\n",
    "        return self.gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the class for the complete computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComputationalGraph():\n",
    "    '''The computational graph and its forward and backward functions'''\n",
    "    \n",
    "    def __init__(self):\n",
    "        # create the whole graph by creating instances of the above defined classes\n",
    "        # since we want to use them for the forward and backward operation, they must have self keyword\n",
    "        # for example...\n",
    "        self.sigm1 = Sigmoid()\n",
    "        # fill out the rest, refer to the computational graph you have plotted for 1.a\n",
    "        self.add1 = Addition()\n",
    "        self.sq1 = Square()\n",
    "        self.add2 = Addition()\n",
    "        self.rec1 = Reciprocal()\n",
    "        self.sigm2 = Sigmoid()\n",
    "        self.add3 = Addition()\n",
    "        self.mul1 = Multiplication()\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        # now connect all the nodes\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        \n",
    "        z0 = self.sigm1.forward(self.x)\n",
    "        \n",
    "        z1 = self.add1.forward(self.x, self.y)\n",
    "        z1 = self.sq1.forward(z1)\n",
    "        z1 = self.add2.forward(z0, z1)\n",
    "        z1 = self.rec1.forward(z1)\n",
    "        \n",
    "        z2 = self.sigm2.forward(self.y)\n",
    "        z2 = self.add3.forward(self.x, z2)\n",
    "        \n",
    "        self.result = self.mul1.forward(z1, z2)\n",
    "        \n",
    "        return self.result\n",
    "        \n",
    "        \n",
    "    def backward(self):\n",
    "        upstream_gradient = 1\n",
    "        \n",
    "        grad_mul1 = self.mul1.backward(upstream_gradient)\n",
    "        \n",
    "        grad_rec1 = self.rec1.backward(grad_mul1[0])\n",
    "        grad_add2 = self.add2.backward(grad_rec1)\n",
    "        grad_sigm1 = self.sigm1.backward(grad_add2)\n",
    "        \n",
    "        grad_sq1 = self.sq1.backward(grad_add2)\n",
    "        grad_add1 = self.add1.backward(grad_sq1)\n",
    "        \n",
    "        grad_add3 = self.add3.backward(grad_mul1[1])\n",
    "        grad_sigm2 = self.sigm2.backward(grad_add3)\n",
    "        \n",
    "        self.grad_x = grad_sigm1 + grad_add1 + grad_add3\n",
    "        self.grad_y = grad_add1 + grad_sigm2\n",
    "        \n",
    "        return [self.grad_x, self.grad_y]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the defined graph to do some actual computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of the function is 1.5456448841066441\n",
      "The gradients for x and y are [2.0595697955721652, 1.5922327514838093]\n"
     ]
    }
   ],
   "source": [
    "x = 3\n",
    "y = -4\n",
    "my_graph = ComputationalGraph()\n",
    "\n",
    "result = my_graph.forward(x, y)\n",
    "print('The result of the function is {}'.format(result))\n",
    "\n",
    "gradients = my_graph.backward()\n",
    "print('The gradients for x and y are {}'.format(gradients))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
